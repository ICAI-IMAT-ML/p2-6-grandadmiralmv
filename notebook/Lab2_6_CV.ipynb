{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9d2nuPDW4ZUK"
   },
   "source": [
    "# Laboratory 2.6: LOO + k-Fold Cross Validation\n",
    "\n",
    "In this practice we will implement one of the main techniques to prevent overfitting when training a model: **cross-validation**.\n",
    "\n",
    "In addition, we will be using the following libraries:\n",
    "- Data management:\n",
    "    - [numpy](https://numpy.org/)\n",
    "- Modelling and scoring:\n",
    "    - [scikit-learn](https://scikit-learn.org)\n",
    "- Plotting:\n",
    "    - [matplotlib](https://matplotlib.org/)\n",
    "    \n",
    "### **All the things you need to do are marked by a \"TODO\" comment nearby. Make sure you *read carefully everything before working* and solve each point before submitting your solution.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "_d4_PT3O4ZUM"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import os\n",
    "import sys\n",
    "# Get the absolute path of the project root\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "\n",
    "# Add it to sys.path\n",
    "sys.path.insert(0, project_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k5wT1jq44ZUM"
   },
   "source": [
    "In the following cell import the training (`training.dat`) and test (`test.dat`) dataset. We recommend you to use the `np.loadtxt()` function.\n",
    "\n",
    "You will need to create the `X_train`, `y_train`, `X_test` and `y_test` variables. Take into account that each dataset have 10 variables, where the last one is the output variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "tl8HWknw4ZUN"
   },
   "outputs": [],
   "source": [
    "# TODO: Load training.dat and text.dat and create X_train, y_train, X_test and y_test\n",
    "train = np.loadtxt(\"training.dat\",delimiter=\",\")\n",
    "X_train = [i[0:9]for i in train]\n",
    "y_train = [i[9] for i in train]\n",
    "test = np.loadtxt(\"test.dat\",delimiter=\",\")\n",
    "X_test = [i[0:9]for i in test]\n",
    "y_test = [i[9] for i in test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XE4EMflX4ZUN"
   },
   "source": [
    "With this data you are going to train the optimal `KNeighborsClassifier` model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5iVd68pH4ZUN"
   },
   "source": [
    "### Initial guess\n",
    "\n",
    "As you have no idea of what the optimal value of `n_neighbors` is, trust your professors and use `n_neighbors = 4` to train your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "CMhqi60d4ZUN"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unknown label type: continuous. Maybe you are trying to fit a classifier, which expects discrete classes on a regression target with continuous values.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# TODO: Train a KNeighborsClassifier model with n_neighbors=4\u001b[39;00m\n\u001b[0;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m KNeighborsClassifier(n_neighbors\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n",
      "File \u001b[1;32mc:\\Users\\velaz\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\velaz\\anaconda3\\Lib\\site-packages\\sklearn\\neighbors\\_classification.py:228\u001b[0m, in \u001b[0;36mKNeighborsClassifier.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;66;03m# KNeighborsClassifier.metric is not validated yet\u001b[39;00m\n\u001b[0;32m    208\u001b[0m     prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    209\u001b[0m )\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y):\n\u001b[0;32m    211\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Fit the k-nearest neighbors classifier from the training dataset.\u001b[39;00m\n\u001b[0;32m    212\u001b[0m \n\u001b[0;32m    213\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    226\u001b[0m \u001b[38;5;124;03m        The fitted k-nearest neighbors classifier.\u001b[39;00m\n\u001b[0;32m    227\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 228\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit(X, y)\n",
      "File \u001b[1;32mc:\\Users\\velaz\\anaconda3\\Lib\\site-packages\\sklearn\\neighbors\\_base.py:480\u001b[0m, in \u001b[0;36mNeighborsBase._fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    478\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutputs_2d_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 480\u001b[0m check_classification_targets(y)\n\u001b[0;32m    481\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_ \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    482\u001b[0m \u001b[38;5;66;03m# Using `dtype=np.intp` is necessary since `np.bincount`\u001b[39;00m\n\u001b[0;32m    483\u001b[0m \u001b[38;5;66;03m# (called in _classification.py) fails when dealing\u001b[39;00m\n\u001b[0;32m    484\u001b[0m \u001b[38;5;66;03m# with a float64 array on 32bit systems.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\velaz\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\multiclass.py:215\u001b[0m, in \u001b[0;36mcheck_classification_targets\u001b[1;34m(y)\u001b[0m\n\u001b[0;32m    207\u001b[0m y_type \u001b[38;5;241m=\u001b[39m type_of_target(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    208\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_type \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    210\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    213\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultilabel-sequences\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    214\u001b[0m ]:\n\u001b[1;32m--> 215\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    216\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown label type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Maybe you are trying to fit a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    217\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclassifier, which expects discrete classes on a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    218\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mregression target with continuous values.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    219\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Unknown label type: continuous. Maybe you are trying to fit a classifier, which expects discrete classes on a regression target with continuous values."
     ]
    }
   ],
   "source": [
    "# TODO: Train a KNeighborsClassifier model with n_neighbors=4\n",
    "model = KNeighborsClassifier(n_neighbors= 4)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oo_wQZs94ZUN"
   },
   "source": [
    "Now, calculate the accuracy of the model for the training and test sets using the `accuracy_score` function from `scikit-learn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-IUy6tE_4ZUO"
   },
   "outputs": [],
   "source": [
    "# TODO: Calculate accuracy in training and test for KNN with k=4\n",
    "acc_train = None\n",
    "acc_test = None\n",
    "print(f'Accuracy, train = {acc_train} test = {acc_test}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RXg2LdcP4ZUO"
   },
   "source": [
    "**What is happening with this value of `n_neighbors`?**\n",
    "> Write your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u_e90-Il4ZUO"
   },
   "source": [
    "### Damage control\n",
    "It seems that `n_neighbors = 4` overfits for this dataset. Let's try to correct this and use `n_neighbors = 20`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e0LAvjGV4ZUO"
   },
   "outputs": [],
   "source": [
    "# TODO: Train a KNeighborsClassifier model with n_neighbors=20\n",
    "model = None\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VCHjp-nM4ZUO"
   },
   "source": [
    "Calculate again the accuracy of training and test sets for this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DuU365Gf4ZUP"
   },
   "outputs": [],
   "source": [
    "# TODO: Calculate accuracy in training and test for KNN with k=20\n",
    "acc_train = None\n",
    "acc_test = None\n",
    "print(f'Accuracy, train = {acc_train} test = {acc_test}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fm-IgXSv4ZUP"
   },
   "source": [
    "It seems that the accuracy of the test set has improved, but how can we be sure that `n_neighbors=20` is the optimal value?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yRUev3Lm4ZUP"
   },
   "source": [
    "### Obtaining optimal value for hyperparameters\n",
    "\n",
    "We could keep trying with different values for `n_neighbors` until we find the optimal one. However, this strategy is unfeasible for real datasets. So how can we obtain a reasonable optimal value for the hyperparameters of a model?\n",
    "\n",
    "We can leverage the power of cross-validation:\n",
    "\n",
    "<center> <img title=\"5-Fold Cross-Validation \" alt=\"cross-validation\" src=\"https://miro.medium.com/v2/resize:fit:1200/1*AAwIlHM8TpAVe4l2FihNUQ.png\"> </center>\n",
    "\n",
    "Cross-validation give us a notion of the generalization error (i.e., test error) using parts of the training set as a validation set. This validation set is data that the trained model has not seen before, so if the model performs well in this part of the dataset it should generalize well in unseen data. However, as a fortunate partition might happen and generalization error might be underestimated, instead of using a single validation set we use K validation sets and we use the mean error in this K sets as the CV-error. This CV-error should give us a reliable estimation of the generalization error.\n",
    "\n",
    "But, how can we use it to obtain the optimal hyperparameters of the model? If the CV-error is an estimation of the generalization error, the hyperparameter values with least CV-error would result in the least generalization error. As CV-error can be computed during training, we obtain a faster way to obtain the optimal values of the hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CBaSPWdl4ZUP"
   },
   "source": [
    "Now that we know why we want cross validation, implement the `cross_validation` function. This function shall implement two cross validation methods:\n",
    "- K-Fold cross validation\n",
    "- Leave-one-out cross validation\n",
    "\n",
    "Check this [link](https://machinelearningmastery.com/k-fold-cross-validation/) to know the details of each method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cn5P9ojg4ZUP"
   },
   "outputs": [],
   "source": [
    "from src.Lab2_6_CV import cross_validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fmh7jiV-4ZUP"
   },
   "source": [
    "Now that we have the `cross_validation` function implemented, let's check which is the optimal value for `n_neighbors` in this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EWzVt_1M4ZUQ"
   },
   "outputs": [],
   "source": [
    "# Initialize lists to store mean scores and standard deviations for each value of k\n",
    "mean_scores = []\n",
    "std_scores = []\n",
    "\n",
    "# Define the range of k values to test\n",
    "k_values = range(4, 80)\n",
    "\n",
    "# TODO: Loop through each value of k, obtaining the mean accuracy score and the standard deviation of the accuracy score in cross validation\n",
    "\n",
    "# TODO: Find the highest score and the corresponding optimal k\n",
    "highest_score = None\n",
    "optimal_k = None\n",
    "\n",
    "print(f\"Optimal value of k: {optimal_k} with a score of {highest_score:.2f}\")\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.errorbar(k_values, mean_scores, yerr=std_scores, fmt='-o', ecolor='r', capsize=5, capthick=2, markersize=5, label='CV Score +/- std dev')\n",
    "plt.axvline(x=optimal_k, linestyle='--', color='k', label=f'Optimal k: {optimal_k}')\n",
    "\n",
    "plt.title('kNN Model Complexity: Cross-Validation Scores')\n",
    "plt.xlabel('Number of Neighbors (k)')\n",
    "plt.ylabel('CV Mean Score')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1ffuliia4ZUQ"
   },
   "source": [
    "Now that we know the optimal value for `n_neighbors`, let's train a KNN model with this value of hyperparameter and check if the generalization error has improved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "--8eDkp_4ZUQ"
   },
   "outputs": [],
   "source": [
    "# TODO: Train model with k=optimal_k\n",
    "model = None\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3Kb0wRR24ZUQ"
   },
   "outputs": [],
   "source": [
    "# TODO: Calculate accuracy in training and test for KNN with k=optimal_k\n",
    "acc_train = None\n",
    "acc_test = None\n",
    "print(f'Accuracy, train = {acc_train} test = {acc_test}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TkGTPUjt4ZUR"
   },
   "source": [
    "### Sensitivity analysis\n",
    "- Does the number of folds affects the optimal value of `n_neighbors`? Why or why not?\n",
    "> Write your answer here\n",
    "\n",
    "- What happens with the computational time if you increment the number of folds?\n",
    "> Write your answer here\n",
    "\n",
    "- Does it worth to increase the number of folds? Is the CV error a better proxy of the test set?\n",
    "> Write your answer here"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
